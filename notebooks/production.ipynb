{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea210c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2533ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAIR-PRICE BRAZILIAN HEALTH DATA PIPELINE\n",
      "============================================================\n",
      "Project root: /home/victor-jose/Documents/projetos/DGU/DGU45/fair-price\n",
      "Status: Production pipeline initialized\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PROJECT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Setup project paths\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "src_path = project_root / 'src'\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import Fair-Price modules\n",
    "from config.settings import get_config\n",
    "from utils.logger import get_extraction_logger, get_standardization_logger\n",
    "from extraction.extractors import OpenDataSUSExtractor\n",
    "from standardization.cleaners import clean_dataframe\n",
    "from standardization.validators import validate_dataframe, get_validation_summary\n",
    "from consolidation.consolidators import HealthDataConsolidator\n",
    "\n",
    "print(\"FAIR-PRICE BRAZILIAN HEALTH DATA PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(\"Status: Production pipeline initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae37041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nData directories configured:\n",
      "  Raw data: /home/victor-jose/Documents/projetos/DGU/DGU45/fair-price/data/raw\n",
      "  Processed data: /home/victor-jose/Documents/projetos/DGU/DGU45/fair-price/data/processed\n",
      "  Output data: /home/victor-jose/Documents/projetos/DGU/DGU45/fair-price/data/output\n"
     ]
    }
   ],
   "source": [
    "# Initialize configuration and loggers\n",
    "config = get_config()\n",
    "extraction_logger = get_extraction_logger()\n",
    "standardization_logger = get_standardization_logger()\n",
    "\n",
    "print(f\"\\\\nData directories configured:\")\n",
    "print(f\"  Raw data: {config.raw_data_dir}\")\n",
    "print(f\"  Processed data: {config.processed_data_dir}\")\n",
    "print(f\"  Output data: {config.output_data_dir}\")\n",
    "\n",
    "# Ensure directories exist\n",
    "config.processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "config.output_data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bfc6151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "PHASE 1: DATA EXTRACTION FROM OPENDATASUS\n",
      "============================================================\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO - üè• OpenDataSUS Extractor initialized\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO - üìÅ Output directory: /home/victor-jose/Documents/projetos/DGU/DGU45/fair-price/data/raw\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO - üéØ Target years: [2020, 2021, 2022, 2023, 2024]\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO - üöÄ Starting Complete multi-year extraction\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO - üöÄ Starting extraction for years: [2020, 2021, 2022, 2023, 2024]\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO - üîç Validating connection to OpenDataSUS...\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO - ‚úÖ Connection to OpenDataSUS validated successfully\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO - üìã Existing files status:\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO -    ‚ùå 2020: Not found\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO -    ‚ùå 2021: Not found\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO -    ‚ùå 2022: Not found\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO -    ‚ùå 2023: Not found\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO -    ‚ùå 2024: Not found\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO - üöÄ Starting CSV download links discovery\n",
      "2025-06-29 22:51:12 - fair_price.extraction - INFO - üîç Scraping BPS page for CSV links...\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO - üìã Found 89 total links on page\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO -    üìÖ Found 2024: https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/BPS/csv/2024.csv.zip\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO -    üìÖ Found 2023: https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/BPS/csv/2023.csv.zip\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO -    üìÖ Found 2022: https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/BPS/csv/2022.csv.zip\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO -    üìÖ Found 2021: https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/BPS/csv/2021.csv.zip\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO -    üìÖ Found 2020: https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/BPS/csv/2020.csv.zip\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO - ‚úÖ Discovered 5 CSV download links\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO - ‚úÖ CSV download links discovery completed: 5 items (0.41s)\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO - üîç Validating download URLs...\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO - üìä Downloading CSV files: 0/5 items\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO - üöÄ Starting CSV file download and extraction\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO - üì• Downloading 2024.csv from https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/BPS/csv/2024.csv.zip\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO -    üì¶ Extracting ZIP file for 2024...\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO -    ‚úÖ ZIP downloaded: 0.7MB\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO -    üìÑ Extracting: 2024.csv\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO -    ‚úÖ Successfully saved: 2024.csv (7.6MB)\n",
      "2025-06-29 22:51:13 - fair_price.extraction - INFO - ‚úÖ CSV file download and extraction completed (0.32s)\n",
      "2025-06-29 22:51:14 - fair_price.extraction - INFO - üìà Downloading CSV files: 1/5 items (20.0%) - 1.3s elapsed\n",
      "2025-06-29 22:51:14 - fair_price.extraction - INFO - üöÄ Starting CSV file download and extraction\n",
      "2025-06-29 22:51:14 - fair_price.extraction - INFO - üì• Downloading 2023.csv from https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/BPS/csv/2023.csv.zip\n",
      "2025-06-29 22:51:14 - fair_price.extraction - INFO -    üì¶ Extracting ZIP file for 2023...\n",
      "2025-06-29 22:51:15 - fair_price.extraction - INFO -    ‚úÖ ZIP downloaded: 1.4MB\n",
      "2025-06-29 22:51:15 - fair_price.extraction - INFO -    üìÑ Extracting: 2023.csv\n",
      "2025-06-29 22:51:15 - fair_price.extraction - INFO -    ‚úÖ Successfully saved: 2023.csv (12.1MB)\n",
      "2025-06-29 22:51:15 - fair_price.extraction - INFO - ‚úÖ CSV file download and extraction completed (0.32s)\n",
      "2025-06-29 22:51:16 - fair_price.extraction - INFO - üìà Downloading CSV files: 2/5 items (40.0%) - 2.6s elapsed\n",
      "2025-06-29 22:51:16 - fair_price.extraction - INFO - üöÄ Starting CSV file download and extraction\n",
      "2025-06-29 22:51:16 - fair_price.extraction - INFO - üì• Downloading 2022.csv from https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/BPS/csv/2022.csv.zip\n",
      "2025-06-29 22:51:16 - fair_price.extraction - INFO -    üì¶ Extracting ZIP file for 2022...\n",
      "2025-06-29 22:51:16 - fair_price.extraction - INFO -    ‚úÖ ZIP downloaded: 3.4MB\n",
      "2025-06-29 22:51:16 - fair_price.extraction - INFO -    üìÑ Extracting: 2022.csv\n",
      "2025-06-29 22:51:16 - fair_price.extraction - INFO -    ‚úÖ Successfully saved: 2022.csv (40.0MB)\n",
      "2025-06-29 22:51:16 - fair_price.extraction - INFO - ‚úÖ CSV file download and extraction completed (0.50s)\n",
      "2025-06-29 22:51:17 - fair_price.extraction - INFO - üìà Downloading CSV files: 3/5 items (60.0%) - 4.1s elapsed\n",
      "2025-06-29 22:51:17 - fair_price.extraction - INFO - üöÄ Starting CSV file download and extraction\n",
      "2025-06-29 22:51:17 - fair_price.extraction - INFO - üì• Downloading 2021.csv from https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/BPS/csv/2021.csv.zip\n",
      "2025-06-29 22:51:17 - fair_price.extraction - INFO -    üì¶ Extracting ZIP file for 2021...\n",
      "2025-06-29 22:51:17 - fair_price.extraction - INFO -    ‚úÖ ZIP downloaded: 3.6MB\n",
      "2025-06-29 22:51:17 - fair_price.extraction - INFO -    üìÑ Extracting: 2021.csv\n",
      "2025-06-29 22:51:18 - fair_price.extraction - INFO -    ‚úÖ Successfully saved: 2021.csv (41.1MB)\n",
      "2025-06-29 22:51:18 - fair_price.extraction - INFO - ‚úÖ CSV file download and extraction completed (0.43s)\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO - üìà Downloading CSV files: 4/5 items (80.0%) - 5.6s elapsed\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO - üöÄ Starting CSV file download and extraction\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO - üì• Downloading 2020.csv from https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/BPS/csv/2020.csv.zip\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO -    üì¶ Extracting ZIP file for 2020...\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO -    ‚úÖ ZIP downloaded: 3.7MB\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO -    üìÑ Extracting: 2020.csv\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO -    ‚úÖ Successfully saved: 2020.csv (41.3MB)\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO - ‚úÖ CSV file download and extraction completed (0.38s)\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO - üìà Downloading CSV files: 5/5 items (100.0%) - 6.0s elapsed\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO - üéØ Downloading CSV files completed: 5/5 items (5.95s)\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO - ‚úÖ Extraction completed: 5/5 files\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO - üíæ Total downloaded: 142.2MB\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO -    üìÑ 2024: 7.6MB\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO -    üìÑ 2023: 12.1MB\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO -    üìÑ 2022: 40.0MB\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO -    üìÑ 2021: 41.1MB\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO -    üìÑ 2020: 41.3MB\n",
      "2025-06-29 22:51:19 - fair_price.extraction - INFO - ‚úÖ Complete multi-year extraction completed: 5 items (7.02s)\n",
      "‚úÖ Extraction completed successfully (7.02s)\n",
      "   Files extracted: 5\n",
      "\\nExtracted files (5):\n",
      "  2020.csv: 41.3 MB\n",
      "  2021.csv: 41.1 MB\n",
      "  2022.csv: 40.0 MB\n",
      "  2023.csv: 12.1 MB\n",
      "  2024.csv: 7.6 MB\n",
      "Total extracted data: 142.2 MB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1: DATA EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 1: DATA EXTRACTION FROM OPENDATASUS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "extraction_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Initialize and run extractor\n",
    "    extractor = OpenDataSUSExtractor()\n",
    "    extraction_report = extractor.extract_all_years()\n",
    "    \n",
    "    extraction_time = time.time() - extraction_start_time\n",
    "    \n",
    "    # ‚úÖ Success logging\n",
    "    print(f\"‚úÖ Extraction completed successfully ({extraction_time:.2f}s)\")\n",
    "    print(f\"   Files extracted: {len(extraction_report)}\")\n",
    "    \n",
    "    # List available files\n",
    "    csv_files = list(config.raw_data_dir.glob(\"*.csv\"))\n",
    "    print(f\"\\\\nExtracted files ({len(csv_files)}):\")\n",
    "    total_size_mb = 0\n",
    "    for csv_file in sorted(csv_files):\n",
    "        size_mb = csv_file.stat().st_size / (1024 * 1024)\n",
    "        total_size_mb += size_mb\n",
    "        print(f\"  {csv_file.name}: {size_mb:.1f} MB\")\n",
    "    print(f\"Total extracted data: {total_size_mb:.1f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    extraction_logger.error(f\"Extraction failed: {str(e)}\")\n",
    "    print(f\"‚ùå Extraction failed: {str(e)}\")\n",
    "    csv_files = list(config.raw_data_dir.glob(\"*.csv\"))\n",
    "    print(f\"Using existing files: {len(csv_files)} CSV files found\")\n",
    "    extraction_time = time.time() - extraction_start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa55e663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "PHASE 2: DATA STANDARDIZATION\n",
      "============================================================\n",
      "Standardizing 5 CSV files...\n",
      "2025-06-29 22:52:05 - fair_price.standardization - INFO - Processing 2020.csv...\n",
      "2025-06-29 22:52:05 - fair_price.standardization - INFO - Loaded 2020.csv: 71,227 rows\n",
      "  ‚úÖ 2020.csv: 99.8% quality (3.36s)\n",
      "2025-06-29 22:52:08 - fair_price.standardization - INFO - Processing 2021.csv...\n",
      "2025-06-29 22:52:08 - fair_price.standardization - INFO - Loaded 2021.csv: 70,893 rows\n",
      "  ‚úÖ 2021.csv: 100.0% quality (4.60s)\n",
      "2025-06-29 22:52:13 - fair_price.standardization - INFO - Processing 2022.csv...\n",
      "2025-06-29 22:52:13 - fair_price.standardization - INFO - Loaded 2022.csv: 69,028 rows\n",
      "  ‚úÖ 2022.csv: 99.6% quality (2.85s)\n",
      "2025-06-29 22:52:15 - fair_price.standardization - INFO - Processing 2023.csv...\n",
      "2025-06-29 22:52:16 - fair_price.standardization - INFO - Loaded 2023.csv: 37,522 rows\n",
      "2025-06-29 22:52:16 - fair_price.standardization - INFO - Added 'ano' column with value 2023\n",
      "  ‚úÖ 2023.csv: 98.9% quality (2.19s)\n",
      "2025-06-29 22:52:18 - fair_price.standardization - INFO - Processing 2024.csv...\n",
      "2025-06-29 22:52:18 - fair_price.standardization - INFO - Loaded 2024.csv: 24,635 rows\n",
      "2025-06-29 22:52:18 - fair_price.standardization - INFO - Added 'ano' column with value 2024\n",
      "  ‚úÖ 2024.csv: 97.5% quality (1.44s)\n",
      "\\n‚úÖ Standardization completed successfully (14.48s)\n",
      "   Files processed: 5/5\n",
      "   Total records: 273,305 ‚Üí 273,305\n",
      "   Data preservation: 100.0%\n",
      "   Average quality score: 99.2%\n",
      "   High quality rows: 270,709\n",
      "\\nStandardized files (5):\n",
      "  2020.csv: 22.8 MB\n",
      "  2021.csv: 25.2 MB\n",
      "  2022.csv: 22.3 MB\n",
      "  2023.csv: 12.2 MB\n",
      "  2024.csv: 7.7 MB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: DATA STANDARDIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: DATA STANDARDIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class ProductionStandardizationProcessor:\n",
    "    \"\"\"Production processor using modular Fair-Price functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "    \n",
    "    def load_csv_with_encoding(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV with Brazilian encoding handling.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='latin-1', sep=';')\n",
    "            self.logger.info(f\"Loaded {file_path.name}: {len(df):,} rows\")\n",
    "            return df\n",
    "        except Exception:\n",
    "            # Try fallback encodings\n",
    "            for encoding, sep in [('utf-8', ';'), ('latin-1', ','), ('utf-8', ',')]:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, encoding=encoding, sep=sep)\n",
    "                    self.logger.info(f\"Loaded {file_path.name} with {encoding}/{sep}\")\n",
    "                    return df\n",
    "                except Exception:\n",
    "                    continue\n",
    "            raise Exception(f\"Failed to load {file_path.name}\")\n",
    "    \n",
    "    def process_single_file(self, input_path: Path, output_path: Path) -> dict:\n",
    "        \"\"\"Process one CSV file through complete standardization pipeline.\"\"\"\n",
    "        start_time = time.time()\n",
    "        file_name = input_path.name\n",
    "        \n",
    "        self.logger.info(f\"Processing {file_name}...\")\n",
    "        \n",
    "        # Load raw data\n",
    "        df_raw = self.load_csv_with_encoding(input_path)\n",
    "        \n",
    "        # Add year column if missing (extract from filename)\n",
    "        year = int(input_path.stem)\n",
    "        if 'ano' not in [col.lower() for col in df_raw.columns]:\n",
    "            df_raw['ano'] = year\n",
    "            self.logger.info(f\"Added 'ano' column with value {year}\")\n",
    "        \n",
    "        # Apply cleaning using Fair-Price cleaners module\n",
    "        df_cleaned = clean_dataframe(df_raw, self.config)\n",
    "        \n",
    "        # Apply validation using Fair-Price validators module\n",
    "        validation_rules = {'required_fields': ['ano', 'uf']}\n",
    "        df_validated = validate_dataframe(df_cleaned, validation_rules)\n",
    "        \n",
    "        # Generate quality summary\n",
    "        quality_summary = get_validation_summary(df_validated)\n",
    "        \n",
    "        # Save business data (remove validation columns)\n",
    "        business_columns = [col for col in df_validated.columns \n",
    "                          if not col.startswith(('quality_', 'has_valid_'))]\n",
    "        df_output = df_validated[business_columns]\n",
    "        \n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_output.to_csv(output_path, index=False, encoding='utf-8', sep=',')\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'file_name': file_name,\n",
    "            'processing_time': processing_time,\n",
    "            'rows_input': len(df_raw),\n",
    "            'rows_output': len(df_output),\n",
    "            'columns_input': len(df_raw.columns),\n",
    "            'columns_output': len(df_output.columns),\n",
    "            'quality_score': quality_summary['avg_quality_score'],\n",
    "            'high_quality_rows': quality_summary['high_quality_rows']\n",
    "        }\n",
    "        \n",
    "# Initialize and run standardization\n",
    "standardization_start_time = time.time()\n",
    "processor = ProductionStandardizationProcessor(config, standardization_logger)\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found for standardization\")\n",
    "    processing_reports = []\n",
    "else:\n",
    "    print(f\"Standardizing {len(csv_files)} CSV files...\")\n",
    "    processing_reports = []\n",
    "    \n",
    "    for csv_file in sorted(csv_files):\n",
    "        try:\n",
    "            output_file = config.processed_data_dir / csv_file.name\n",
    "            report = processor.process_single_file(csv_file, output_file)\n",
    "            processing_reports.append(report)\n",
    "            \n",
    "            print(f\"  ‚úÖ {report['file_name']}: {report['quality_score']:.1f}% quality ({report['processing_time']:.2f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            standardization_logger.error(f\"Failed to process {csv_file.name}: {str(e)}\")\n",
    "            print(f\"  ‚ùå {csv_file.name}: Failed - {str(e)}\")\n",
    "\n",
    "standardization_time = time.time() - standardization_start_time\n",
    "\n",
    "# Standardization summary\n",
    "if processing_reports:\n",
    "    total_input_rows = sum(r['rows_input'] for r in processing_reports)\n",
    "    total_output_rows = sum(r['rows_output'] for r in processing_reports)\n",
    "    avg_quality_score = sum(r['quality_score'] for r in processing_reports) / len(processing_reports)\n",
    "    total_high_quality = sum(r['high_quality_rows'] for r in processing_reports)\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Standardization completed successfully ({standardization_time:.2f}s)\")\n",
    "    print(f\"   Files processed: {len(processing_reports)}/{len(csv_files)}\")\n",
    "    print(f\"   Total records: {total_input_rows:,} ‚Üí {total_output_rows:,}\")\n",
    "    print(f\"   Data preservation: {(total_output_rows/total_input_rows)*100:.1f}%\")\n",
    "    print(f\"   Average quality score: {avg_quality_score:.1f}%\")\n",
    "    print(f\"   High quality rows: {total_high_quality:,}\")\n",
    "    \n",
    "    # Show processed files\n",
    "    processed_files = list(config.processed_data_dir.glob(\"*.csv\"))\n",
    "    print(f\"\\\\nStandardized files ({len(processed_files)}):\")\n",
    "    for pfile in sorted(processed_files):\n",
    "        size_mb = pfile.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {pfile.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No files were successfully standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce0529d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "PHASE 3: DATA CONSOLIDATION\n",
      "============================================================\n",
      "Consolidating 5 standardized files...\n",
      "  2020.csv: 22.8 MB\n",
      "  2021.csv: 25.2 MB\n",
      "  2022.csv: 22.3 MB\n",
      "  2023.csv: 12.2 MB\n",
      "  2024.csv: 7.7 MB\n",
      "Total input data: 90.2 MB\n",
      "2025-06-29 22:52:47 - fair_price.standardization - INFO - üöÄ Starting complete data consolidation process...\n",
      "2025-06-29 22:52:47 - fair_price.standardization - INFO - üìÇ Loading standardized files from /home/victor-jose/Documents/projetos/DGU/DGU45/fair-price/data/processed\n",
      "2025-06-29 22:52:47 - fair_price.standardization - INFO -    ‚úÖ 2020: 71,227 rows, 20 columns\n",
      "2025-06-29 22:52:47 - fair_price.standardization - INFO -    ‚úÖ 2021: 70,893 rows, 20 columns\n",
      "2025-06-29 22:52:47 - fair_price.standardization - INFO -    ‚úÖ 2022: 69,028 rows, 20 columns\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    ‚úÖ 2023: 37,522 rows, 20 columns\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    ‚úÖ 2024: 24,635 rows, 20 columns\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO - üìä Total loaded: 5 files, 273,305 rows\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO - üîç Validating schema consistency across files...\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    üìä Schema consistency: 81.8%\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    üìã Common columns: 18/22\n",
      "2025-06-29 22:52:48 - fair_price.standardization - WARNING -    ‚ö†Ô∏è  Files have inconsistent columns\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO - üîß Standardizing schemas across all files...\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    üìã Using config unified schema: 21 columns\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    ‚úÖ 2020: standardized to 21 columns\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    ‚úÖ 2021: standardized to 21 columns\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    ‚úÖ 2022: standardized to 21 columns\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    ‚úÖ 2023: standardized to 21 columns\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    ‚úÖ 2024: standardized to 21 columns\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO - üîç Detecting cross-year duplicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor-jose/Documents/projetos/DGU/DGU45/fair-price/src/consolidation/consolidators.py:195: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(combined_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    üìä Total duplicates: 28,213 (10.3%)\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    üîÄ Cross-year duplicates: 0\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO -    üìÖ Same-year duplicates: 0\n",
      "2025-06-29 22:52:48 - fair_price.standardization - INFO - üîß Consolidating data with 'keep_latest' duplicate strategy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor-jose/Documents/projetos/DGU/DGU45/fair-price/src/consolidation/consolidators.py:256: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  consolidated_df = pd.concat(all_dataframes, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -    üìä Consolidation complete:\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -       Input records: 273,305\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -       Output records: 250,789\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -       Reduction: 8.2%\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO - ‚úÖ Validating consolidated dataset...\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -    üìä Final dataset: 250,789 rows √ó 21 columns\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -    üíæ Memory usage: 257.4 MB\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -    ‚úÖ Overall completeness: 78.0%\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -    üìÖ Year distribution:\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -       2020: 71,216 records\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -       2021: 70,814 records\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -       2022: 68,965 records\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -       2023: 27,815 records\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO -       2024: 11,979 records\n",
      "2025-06-29 22:52:49 - fair_price.standardization - INFO - üíæ Saving consolidated data to /home/victor-jose/Documents/projetos/DGU/DGU45/fair-price/data/output\n",
      "2025-06-29 22:52:51 - fair_price.standardization - WARNING - Could not save Parquet format: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
      "2025-06-29 22:52:51 - fair_price.standardization - INFO -    ‚úÖ CSV: consolidated_health_data_20250629_225249.csv (73.5 MB)\n",
      "2025-06-29 22:52:51 - fair_price.standardization - INFO - ‚úÖ Consolidation completed in 3.44 seconds\n",
      "2025-06-29 22:52:51 - fair_price.standardization - INFO - üìä Final dataset: 250,789 records\n",
      "\\n‚úÖ Consolidation completed successfully (3.45s)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PHASE 3: DATA CONSOLIDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 3: DATA CONSOLIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "consolidation_start_time = time.time()\n",
    "\n",
    "# Check for standardized files\n",
    "standardized_files = list(config.processed_data_dir.glob(\"*.csv\"))\n",
    "\n",
    "if not standardized_files:\n",
    "    print(\"‚ùå No standardized files found for consolidation\")\n",
    "    consolidation_report = None\n",
    "else:\n",
    "    print(f\"Consolidating {len(standardized_files)} standardized files...\")\n",
    "    \n",
    "    # Show input files\n",
    "    total_input_size = 0\n",
    "    for sfile in sorted(standardized_files):\n",
    "        size_mb = sfile.stat().st_size / (1024 * 1024)\n",
    "        total_input_size += size_mb\n",
    "        print(f\"  {sfile.name}: {size_mb:.1f} MB\")\n",
    "    print(f\"Total input data: {total_input_size:.1f} MB\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize and run consolidator\n",
    "        consolidator = HealthDataConsolidator(config, standardization_logger)\n",
    "        consolidation_report = consolidator.consolidate_all_data(\n",
    "            input_dir=config.processed_data_dir,\n",
    "            output_dir=config.output_data_dir,\n",
    "            duplicate_strategy=\"keep_latest\"\n",
    "        )\n",
    "        \n",
    "        consolidation_time = time.time() - consolidation_start_time\n",
    "        \n",
    "        print(f\"\\\\n‚úÖ Consolidation completed successfully ({consolidation_time:.2f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        standardization_logger.error(f\"Consolidation failed: {str(e)}\")\n",
    "        print(f\"‚ùå Consolidation failed: {str(e)}\")\n",
    "        consolidation_report = None\n",
    "        consolidation_time = time.time() - consolidation_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8269664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "PIPELINE EXECUTION SUMMARY\n",
      "============================================================\n",
      "Execution times:\n",
      "  Phase 1 (Extraction): 7.02s\n",
      "  Phase 2 (Standardization): 14.48s\n",
      "  Phase 3 (Consolidation): 3.45s\n",
      "  Total pipeline time: 24.96s\n",
      "\\nPhase results:\n",
      "  ‚úÖ Extraction: 5 files (142.2 MB)\n",
      "  ‚úÖ Standardization: 273,305 records (99.2% quality)\n",
      "  ‚úÖ Consolidation: 250,789 final records (78.0% quality)\n",
      "\\nFinal output files:\n",
      "  CSV: consolidated_health_data_20250629_225249.csv (73.5 MB)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE RESULTS SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate total pipeline time\n",
    "total_pipeline_time = extraction_time + standardization_time + (consolidation_time if 'consolidation_time' in locals() else 0)\n",
    "\n",
    "print(f\"Execution times:\")\n",
    "print(f\"  Phase 1 (Extraction): {extraction_time:.2f}s\")\n",
    "print(f\"  Phase 2 (Standardization): {standardization_time:.2f}s\")\n",
    "if 'consolidation_time' in locals():\n",
    "    print(f\"  Phase 3 (Consolidation): {consolidation_time:.2f}s\")\n",
    "print(f\"  Total pipeline time: {total_pipeline_time:.2f}s\")\n",
    "\n",
    "# Phase-by-phase status\n",
    "print(f\"\\\\nPhase results:\")\n",
    "print(f\"  ‚úÖ Extraction: {len(csv_files) if csv_files else 0} files ({total_size_mb:.1f} MB)\")\n",
    "\n",
    "if processing_reports:\n",
    "    print(f\"  ‚úÖ Standardization: {total_output_rows:,} records ({avg_quality_score:.1f}% quality)\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Standardization: Failed\")\n",
    "\n",
    "if consolidation_report:\n",
    "    summary = consolidation_report['consolidation_summary']\n",
    "    final_records = summary['output_records']\n",
    "    quality_score = summary['data_quality_score']\n",
    "    print(f\"  ‚úÖ Consolidation: {final_records:,} final records ({quality_score:.1f}% quality)\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Consolidation: Failed or skipped\")\n",
    "\n",
    "# Final output files\n",
    "if consolidation_report and 'saved_files' in consolidation_report:\n",
    "    print(f\"\\\\nFinal output files:\")\n",
    "    for format_name, file_path in consolidation_report['saved_files'].items():\n",
    "        file_path_obj = Path(file_path)\n",
    "        if file_path_obj.exists():\n",
    "            size_mb = file_path_obj.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  {format_name.upper()}: {file_path_obj.name} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a59dc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "DATA QUALITY ASSESSMENT\n",
      "============================================================\n",
      "Dataset characteristics:\n",
      "  Total records: 250,789\n",
      "  Total columns: 21\n",
      "  Memory usage: 257.4 MB\n",
      "  Overall completeness: 78.0%\n",
      "  Duplicate reduction: 10.3%\n",
      "\\nYear distribution:\n",
      "  2020: 71,216 records (28.4%)\n",
      "  2021: 70,814 records (28.2%)\n",
      "  2022: 68,965 records (27.5%)\n",
      "  2023: 27,815 records (11.1%)\n",
      "  2024: 11,979 records (4.8%)\n",
      "\\nMost complete columns (top 5):\n",
      "  ano: 100.0%\n",
      "  codigo_br: 100.0%\n",
      "  descricao_catmat: 100.0%\n",
      "  generico: 100.0%\n",
      "  compra: 100.0%\n",
      "\\nColumns with <90% completeness (7):\n",
      "  modalidade_compra: 0.0%\n",
      "  unidade_fornecimento: 15.8%\n",
      "  preco_total: 15.9%\n",
      "  cnpj_fabricante: 43.6%\n",
      "  cnpj_instituicao: 44.1%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA QUALITY ASSESSMENT\n",
    "# =============================================================================\n",
    "\n",
    "if consolidation_report:\n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    print(\"DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    final_validation = consolidation_report['final_validation']\n",
    "    duplicate_detection = consolidation_report['duplicate_detection']\n",
    "    \n",
    "    print(f\"Dataset characteristics:\")\n",
    "    print(f\"  Total records: {final_validation['total_rows']:,}\")\n",
    "    print(f\"  Total columns: {final_validation['total_columns']}\")\n",
    "    print(f\"  Memory usage: {final_validation['memory_usage_mb']:.1f} MB\")\n",
    "    print(f\"  Overall completeness: {final_validation['overall_completeness']:.1f}%\")\n",
    "    print(f\"  Duplicate reduction: {duplicate_detection['duplicate_percentage']:.1f}%\")\n",
    "    \n",
    "    # Year distribution\n",
    "    if 'year_distribution' in final_validation:\n",
    "        print(f\"\\\\nYear distribution:\")\n",
    "        year_dist = final_validation['year_distribution']\n",
    "        total_records = sum(year_dist.values())\n",
    "        for year in sorted(year_dist.keys()):\n",
    "            count = year_dist[year]\n",
    "            percentage = (count / total_records) * 100\n",
    "            print(f\"  {year}: {count:,} records ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Column completeness analysis\n",
    "    completeness = final_validation['completeness_by_column']\n",
    "    \n",
    "    # Most complete columns\n",
    "    print(f\"\\\\nMost complete columns (top 5):\")\n",
    "    top_complete = sorted(completeness.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for col, comp in top_complete:\n",
    "        print(f\"  {col}: {comp:.1f}%\")\n",
    "    \n",
    "    # Columns needing attention\n",
    "    incomplete_columns = [(col, comp) for col, comp in completeness.items() if comp < 90]\n",
    "    if incomplete_columns:\n",
    "        print(f\"\\\\nColumns with <90% completeness ({len(incomplete_columns)}):\")\n",
    "        bottom_complete = sorted(incomplete_columns, key=lambda x: x[1])[:5]\n",
    "        for col, comp in bottom_complete:\n",
    "            print(f\"  {col}: {comp:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9883b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fair-Price (envfp)",
   "language": "python",
   "name": "envfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
